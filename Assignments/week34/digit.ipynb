{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/sohamnivargi/Desktop/Code/week34/train.csv')\n",
    "\n",
    "X = df[df.columns[1:len(df.columns)]]\n",
    "Y = df.label\n",
    "\n",
    "X_dev = X[:1000]\n",
    "Y_dev = Y[:1000]\n",
    "\n",
    "X_train = X[1000:]\n",
    "Y_train = Y[1000:]\n",
    "\n",
    "X_dev = X_dev / 255.0\n",
    "X_train = X_train / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    units = [784, 120, 45, 10]\n",
    "    W_params = [[], [], []]\n",
    "    b_params = [[], [], []]\n",
    "    \n",
    "    for i in range(3):\n",
    "        W_params[i] = np.random.rand(units[i], units[i+1])\n",
    "        b_params[i] = np.random.rand(units[i+1], 1)\n",
    "    return W_params, b_params\n",
    "w,b = init_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(inp):\n",
    "    return np.maximum(0,inp)\n",
    "    \n",
    "def softmax(inp):\n",
    "    max_val = np.max(inp, axis=0)\n",
    "    shifted_inp = inp - max_val\n",
    "    exp_val = np.exp(shifted_inp)\n",
    "    soft_max = exp_val / np.sum(exp_val, axis=0, keepdims=True)\n",
    "    return soft_max\n",
    "\n",
    "\n",
    "def one_hot(y):\n",
    "    num_classes = 10\n",
    "    num_examples = len(y)\n",
    "    one_hot = np.zeros((num_classes, num_examples))\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        class_label = y[i]\n",
    "        one_hot[class_label, i] = 1\n",
    "    return one_hot.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(weights, biases, X):\n",
    "\n",
    "    Z = []  \n",
    "    A = []  \n",
    "\n",
    "    num_layers = len(weights)\n",
    "    A_prev = X.T\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        Z_current = (np.dot(weights[i].T, A_prev) + biases[i]).T\n",
    "        \n",
    "        Z.append(Z_current)\n",
    "\n",
    "        if i == num_layers - 1:\n",
    "            A_current = softmax(Z_current)\n",
    "        else:\n",
    "            A_current = relu(Z_current)\n",
    "        \n",
    "        A.append(A_current)\n",
    "        A_prev = A_current.T\n",
    "        \n",
    "    return Z, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(Z1,Z2,Z3,W1,W2,W3,A1,A2,A3,x,y):\n",
    "    m = x.shape[1]\n",
    "    \n",
    "    dZ3 = A3-y\n",
    "    \n",
    "    dW3 = (1 / m) * np.dot(dZ3.T, A2)\n",
    "    \n",
    "    db3 = (1 / m) * np.sum(dZ3.T, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    dA2 = (np.dot(W3, dZ3.T)).T\n",
    "    \n",
    "    \n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    \n",
    "    dW2 = (1 / m) * np.dot(dZ2.T, A1)\n",
    "    db2 = (1 / m) * np.sum(dZ2.T, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = (np.dot(W2, dZ2.T)).T\n",
    "    \n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0)) \n",
    "    dW1 = (1 / m) * np.dot(dZ1.T, x)\n",
    "    db1 = (1 / m) * np.sum(dZ1.T, axis=1, keepdims=True)\n",
    "   \n",
    "    \n",
    "    return dW1.T, db1, dW2.T, db2, dW3.T, db3\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
    "    \n",
    "    W1 = np.subtract(W1, alpha * dW1)\n",
    "    b1 = np.subtract(b1, alpha * db1)\n",
    "    W2 = np.subtract(W2, alpha * dW2)\n",
    "    b2 = np.subtract(b2, alpha * db2)\n",
    "    W3 = np.subtract(W3, alpha * dW3)\n",
    "    b3 = np.subtract(b3, alpha * db3)\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(A3):\n",
    "    \n",
    "    predicted_labels = np.argmax(A3, axis=1)\n",
    "    return predicted_labels\n",
    "\n",
    "def get_accuracy(predicted_labels, true_labels):\n",
    "    \n",
    "    num_correct = np.sum(predicted_labels == true_labels)\n",
    "    accuracy = num_correct / true_labels.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Accuracy: 0.09817073170731708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/z121n0b53dlgwz_hcn7tbclc0000gn/T/ipykernel_38293/2272830888.py:6: RuntimeWarning: invalid value encountered in subtract\n",
      "  shifted_inp = inp - max_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 Accuracy: 0.09817073170731708\n",
      "Iteration: 20 Accuracy: 0.09817073170731708\n",
      "Iteration: 30 Accuracy: 0.09817073170731708\n",
      "Iteration: 40 Accuracy: 0.09817073170731708\n",
      "Iteration: 50 Accuracy: 0.09817073170731708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1137], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m Y_train \u001b[39m=\u001b[39m Y_train\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m X_train \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m W_f, b_f \u001b[39m=\u001b[39m gradient_descent(X_train,Y_train,\u001b[39m0.1\u001b[39;49m,\u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[1137], line 10\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X_train, Y_train, alpha, num_iterations)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y)):\n\u001b[0;32m---> 10\u001b[0m         Z, A \u001b[39m=\u001b[39m forward_propagation(W,b,X_train)\n\u001b[1;32m     12\u001b[0m         dW1, db1, dW2, db2, dW3, db3 \u001b[39m=\u001b[39m back_prop(Z[\u001b[39m0\u001b[39m], Z[\u001b[39m1\u001b[39m],Z[\u001b[39m2\u001b[39m], W[\u001b[39m0\u001b[39m], W[\u001b[39m1\u001b[39m], W[\u001b[39m2\u001b[39m], A[\u001b[39m0\u001b[39m], A[\u001b[39m1\u001b[39m], A[\u001b[39m2\u001b[39m], \n\u001b[1;32m     13\u001b[0m                                                 X_train, y)\n\u001b[1;32m     15\u001b[0m         W[\u001b[39m0\u001b[39m], b[\u001b[39m0\u001b[39m], W[\u001b[39m1\u001b[39m], b[\u001b[39m1\u001b[39m], W[\u001b[39m2\u001b[39m], b[\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m update_params(W[\u001b[39m0\u001b[39m], b[\u001b[39m0\u001b[39m], W[\u001b[39m1\u001b[39m], b[\u001b[39m1\u001b[39m], W[\u001b[39m2\u001b[39m], b[\u001b[39m2\u001b[39m], \n\u001b[1;32m     16\u001b[0m                                                            dW1, db1, dW2, db2, dW3, db3, alpha)\n",
      "Cell \u001b[0;32mIn[1133], line 10\u001b[0m, in \u001b[0;36mforward_propagation\u001b[0;34m(weights, biases, X)\u001b[0m\n\u001b[1;32m      7\u001b[0m A_prev \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mT\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_layers):\n\u001b[0;32m---> 10\u001b[0m     Z_current \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39;49mdot(weights[i]\u001b[39m.\u001b[39;49mT, A_prev) \u001b[39m+\u001b[39m biases[i])\u001b[39m.\u001b[39mT\n\u001b[1;32m     12\u001b[0m     Z\u001b[39m.\u001b[39mappend(Z_current)\n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m num_layers \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def gradient_descent(X_train, Y_train, alpha=0.1, num_iterations=1000):\n",
    "\n",
    "    W,b = init_params()\n",
    "    \n",
    "    y = one_hot(Y_train)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        for j in range(len(y)):\n",
    "        \n",
    "            Z, A = forward_propagation(W,b,X_train)\n",
    "            \n",
    "            dW1, db1, dW2, db2, dW3, db3 = back_prop(Z[0], Z[1],Z[2], W[0], W[1], W[2], A[0], A[1], A[2], \n",
    "                                                    X_train, y)\n",
    "            \n",
    "            W[0], b[0], W[1], b[1], W[2], b[2] = update_params(W[0], b[0], W[1], b[1], W[2], b[2], \n",
    "                                                               dW1, db1, dW2, db2, dW3, db3, alpha)\n",
    "        #xx\n",
    "        \n",
    "            if j % 10 == 0:\n",
    "                predicted_labels = get_prediction(A[2])\n",
    "                accuracy = get_accuracy(predicted_labels, Y_train)\n",
    "                print(\"Iteration:\", j, \"Accuracy:\", accuracy)\n",
    "        \n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            predicted_labels = get_prediction(A[2])\n",
    "            accuracy = get_accuracy(predicted_labels, Y_train)\n",
    "            print(\" 10th Iteration:\", i, \"Accuracy:\", accuracy)\n",
    "\n",
    "    \n",
    "    return W, b\n",
    "Y_train = Y_train.reset_index(drop=True)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "W_f, b_f = gradient_descent(X_train,Y_train,0.1,1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W, b):\n",
    "    \n",
    "    Zx,Ax = forward_propagation(W,b,X)\n",
    "    return Ax[2]\n",
    "\n",
    "def test_prediction(index, W, b):\n",
    "    \n",
    "    X = X_train[index]\n",
    "    true_label = Y_train[index]\n",
    "    prediction = make_predictions(X, W, b)\n",
    "\n",
    "    plt.imshow(X.reshape((28, 28)), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Prediction: {prediction}, True Label: {true_label}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
